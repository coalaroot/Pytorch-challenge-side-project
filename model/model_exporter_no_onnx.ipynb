{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools.import_pb_to_tensorboard import import_to_tensorboard\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.contrib.lite.python.lite import TFLiteConverter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: 'NINE',\n",
    "    1: 'ZERO',\n",
    "    2: 'SEVEN',\n",
    "    3: 'SIX',\n",
    "    4: 'ONE',\n",
    "    5: 'EIGHT',\n",
    "    6: 'FOUR',\n",
    "    7: 'THREE',\n",
    "    8: 'TWO',\n",
    "    9: 'FIVE', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # Ignore all the warning messages in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x12B0DB978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open('test.jpg').convert('L')\n",
    "display(img) # show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture without Dropout layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(1, 8, 3, padding=1)      \n",
    "        self.conv1_2 = nn.Conv2d(8, 16, 3, padding=1)      \n",
    "        self.conv1_3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2) \n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(32, 40, 3, padding=1)      \n",
    "        self.conv2_2 = nn.Conv2d(40, 48, 3, padding=1)      \n",
    "        self.conv2_3 = nn.Conv2d(48, 64, 3, padding=1)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(64, 72, 3, padding=1)      \n",
    "        self.conv3_2 = nn.Conv2d(72, 80, 3, padding=1)      \n",
    "        self.conv3_3 = nn.Conv2d(80, 96, 3, padding=1)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(96, 128, 3, padding=1)      \n",
    "        self.conv4_2 = nn.Conv2d(128, 192, 3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(192, 256, 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 64x64x1 => 32x32x8\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = F.relu(self.conv1_3(x))\n",
    "        x = F.relu(self.pool1(x))\n",
    "        \n",
    "        # 32x32x8 => 16x16x16\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = F.relu(self.conv2_3(x))\n",
    "        x = F.relu(self.pool2(x))\n",
    "        \n",
    "        # 16x16x16 => 8x8x32\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = F.relu(self.pool3(x))\n",
    "        \n",
    "        # 8x8x32 => 8x8x256\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "\n",
    "        # Reduce Mean & Linear\n",
    "        x = torch.mean(torch.mean(x, dim=-1), dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(32, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(40, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(80, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('./model_sl_3968.pt', map_location=lambda storage, location: storage))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -3.2384,  32.9433,  -6.1564, -18.4737,  -2.4728,   0.2746,  -1.5060,\n",
       "          -14.4440, -26.7159,  -7.6219]], grad_fn=<ThAddmmBackward>), 'ZERO')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test predict\n",
    "x = torch.from_numpy(np.expand_dims(np.expand_dims(np.asarray(img, dtype=np.float32), axis=0), axis=0))\n",
    "y = model(x)\n",
    "\n",
    "y_idx = torch.argmax(y)\n",
    "y, label_dict[int(y_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 3, 3]), torch.Size([8, 3, 3, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1_1.weight.shape, model.conv1_1.weight.permute([0, 2, 3, 1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_op(x, torch_model):    \n",
    "#     # 64x64x1 => 32x32x8\n",
    "#     x = F.relu(self.conv1_1(x))\n",
    "#     x = F.relu(self.conv1_2(x))\n",
    "#     x = F.relu(self.conv1_3(x))\n",
    "#     x = F.relu(self.pool1(x))\n",
    "\n",
    "#     self.conv1_1 = nn.Conv2d(1, 8, 3, padding=1)      \n",
    "#     self.conv1_2 = nn.Conv2d(8, 16, 3, padding=1)      \n",
    "#     self.conv1_3 = nn.Conv2d(16, 24, 3, padding=1)\n",
    "#     self.pool1 = nn.Conv2d(24, 32, 3, padding=1, stride=2) \n",
    "        \n",
    "    # 64x64x1 => 32x32x8\n",
    "    x = tf.layers.conv2d(x, 8, 3, padding='same', activation=tf.nn.relu, name='conv1_1', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_1.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_1.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 16, 3, padding='same', activation=tf.nn.relu, name='conv1_2', trainable=False, \n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_2.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_2.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 32, 3, padding='same', activation=tf.nn.relu, name='conv1_3', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_3.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv1_3.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, name='pool1')\n",
    "    \n",
    "#     # 32x32x8 => 16x16x16\n",
    "#     x = F.relu(self.conv2_1(x))\n",
    "#     x = F.relu(self.conv2_2(x))\n",
    "#     x = F.relu(self.conv2_3(x))\n",
    "#     x = F.relu(self.pool2(x))\n",
    "\n",
    "#     self.conv2_1 = nn.Conv2d(32, 40, 3, padding=1)      \n",
    "#     self.conv2_2 = nn.Conv2d(40, 48, 3, padding=1)      \n",
    "#     self.conv2_3 = nn.Conv2d(48, 56, 3, padding=1)\n",
    "#     self.pool2 = nn.Conv2d(56, 64, 3, padding=1, stride=2)\n",
    "\n",
    "    # 32x32x8 => 16x16x16\n",
    "    x = tf.layers.conv2d(x, 40, 3, padding='same', activation=tf.nn.relu, name='conv2_1', trainable=False, \n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_1.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_1.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 48, 3, padding='same', activation=tf.nn.relu, name='conv2_2', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_2.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_2.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 64, 3, padding='same', activation=tf.nn.relu, name='conv2_3', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_3.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv2_3.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, name='pool2')\n",
    "\n",
    "#     # 16x16x16 => 8x8x32\n",
    "#     x = F.relu(self.conv3_1(x))\n",
    "#     x = F.relu(self.conv3_2(x))\n",
    "#     x = F.relu(self.conv3_3(x))\n",
    "#     x = F.relu(self.pool3(x))\n",
    "\n",
    "#     self.conv3_1 = nn.Conv2d(64, 72, 3, padding=1)      \n",
    "#     self.conv3_2 = nn.Conv2d(72, 80, 3, padding=1)      \n",
    "#     self.conv3_3 = nn.Conv2d(80, 88, 3, padding=1)\n",
    "#     self.pool3 = nn.Conv2d(88, 96, 3, padding=1, stride=2)\n",
    "\n",
    "    # 16x16x16 => 8x8x32\n",
    "    x = tf.layers.conv2d(x, 72, 3, padding='same', activation=tf.nn.relu, name='conv3_1', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_1.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_1.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 80, 3, padding='same', activation=tf.nn.relu, name='conv3_2', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_2.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_2.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 96, 3, padding='same', activation=tf.nn.relu, name='conv3_3', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_3.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv3_3.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, name='pool3')\n",
    "\n",
    "#     # 8x8x32 => 8x8x256\n",
    "#     x = F.relu(self.conv4_1(x))\n",
    "#     x = F.relu(self.conv4_2(x))\n",
    "#     x = F.relu(self.conv4_3(x))\n",
    "\n",
    "#     self.conv4_1 = nn.Conv2d(96, 128, 3, padding=1)      \n",
    "#     self.conv4_2 = nn.Conv2d(128, 192, 3, padding=1)\n",
    "#     self.conv4_3 = nn.Conv2d(192, 256, 3, padding=1)\n",
    "\n",
    "    # 8x8x32 => 8x8x256\n",
    "    x = tf.layers.conv2d(x, 128, 3, padding='same', activation=tf.nn.relu, name='conv4_1', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_1.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_1.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 192, 3, padding='same', activation=tf.nn.relu, name='conv4_2', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_2.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_2.bias.detach().numpy())\n",
    "    )\n",
    "    x = tf.layers.conv2d(x, 256, 3, padding='same', activation=tf.nn.relu, name='conv4_3', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_3.weight.permute([2, 3, 1, 0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.conv4_3.bias.detach().numpy())\n",
    "    )\n",
    "\n",
    "#     x = torch.mean(torch.mean(x, dim=-1), dim=-1)\n",
    "#     x = F.relu(self.fc1(x))\n",
    "#     x = self.fc2(x)\n",
    "\n",
    "#     self.fc1 = nn.Linear(256, 1024)\n",
    "#     self.fc2 = nn.Linear(1024, 10)    \n",
    "\n",
    "    # Reduce Mean & Linear\n",
    "    x = tf.math.reduce_mean(tf.math.reduce_mean(x, axis=2), axis=1)\n",
    "    x = tf.layers.dense(x, 1024, name='fc1', activation=tf.nn.relu, trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.fc1.weight.permute([1,0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.fc1.bias.detach().numpy())\n",
    "    )\n",
    "    y = tf.layers.dense(x, 10, name='fc2', trainable=False,\n",
    "            kernel_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.fc2.weight.permute([1,0]).detach().numpy()),\n",
    "            bias_initializer=lambda x, dtype, partition_info: tf.constant(torch_model.fc2.bias.detach().numpy())\n",
    "    )\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Generate Model\n",
    "input_placeholder = tf.placeholder(tf.float32, shape=(None, 64, 64, 1), name=\"0\")\n",
    "pred_tensor = tf.identity(add_prediction_op(input_placeholder, model), name=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare With PyTorch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [] \n",
    "\n",
    "tx = x.clone()\n",
    "tx = F.relu(model.conv1_1(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv1_2(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv1_3(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.pool1(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "\n",
    "# 32x32x8 => 16x16x16\n",
    "tx = F.relu(model.conv2_1(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv2_2(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv2_3(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.pool2(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "\n",
    "# 16x16x16 => 8x8x32\n",
    "tx = F.relu(model.conv3_1(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv3_2(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv3_3(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.pool3(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "\n",
    "# 8x8x32 => 8x8x256\n",
    "tx = F.relu(model.conv4_1(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv4_2(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "tx = F.relu(model.conv4_3(tx))\n",
    "vals.append(tx.permute(0,2,3,1).detach().numpy())\n",
    "\n",
    "# Reduce Mean & Linear\n",
    "tx = torch.mean(torch.mean(tx, dim=-1), dim=-1)\n",
    "vals.append(tx.permute(1,0).detach().numpy())\n",
    "tx = F.relu(model.fc1(tx))\n",
    "vals.append(tx.permute(1,0).detach().numpy())\n",
    "tx = model.fc2(tx)\n",
    "vals.append(tx.permute(1,0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -3.2383766 ,  32.943295  ,  -6.1564054 , -18.473711  ,\n",
       "          -2.472799  ,   0.27463526,  -1.5059702 , -14.443997  ,\n",
       "         -26.71591   ,  -7.621882  ]], dtype=float32), 'ZERO')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_idx = np.argmax(vals[17])\n",
    "np.transpose(vals[17]),label_dict[int(y_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_x = x.permute([0,2,3,1]).detach().double().numpy()\n",
    "with tf.Session() as sess: \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    y0 = sess.run('pred:0', feed_dict={'0:0':tf_x})\n",
    "    y1 = sess.run(pred_tensor, feed_dict={input_placeholder:tf_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -3.2383754 ,  32.943283  ,  -6.156398  , -18.473707  ,\n",
       "          -2.4727986 ,   0.27463734,  -1.5059701 , -14.443996  ,\n",
       "         -26.715906  ,  -7.621884  ]], dtype=float32), 'ZERO')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0_idx = np.argmax(y0)\n",
    "y0, label_dict[int(y0_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -3.2383754 ,  32.943283  ,  -6.156398  , -18.473707  ,\n",
       "          -2.4727986 ,   0.27463734,  -1.5059701 , -14.443996  ,\n",
       "         -26.715906  ,  -7.621884  ]], dtype=float32), 'ZERO')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_idx = np.argmax(y1)\n",
    "y1, label_dict[int(y1_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 28 variables.\n",
      "INFO:tensorflow:Converted 28 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    output_nodes_name = [\n",
    "        'conv1_1/kernel','conv1_1/bias','conv1_2/kernel','conv1_2/bias','conv1_3/kernel','conv1_3/bias',\n",
    "        'conv2_1/kernel','conv2_1/bias','conv2_2/kernel','conv2_2/bias','conv2_3/kernel','conv2_3/bias',\n",
    "        'conv3_1/kernel','conv3_1/bias','conv3_2/kernel','conv3_2/bias','conv3_3/kernel','conv3_3/bias',\n",
    "        'conv4_1/kernel','conv4_1/bias','conv4_2/kernel','conv4_2/bias','conv4_3/kernel','conv4_3/bias',\n",
    "        'fc1/kernel','fc1/bias','fc2/kernel','fc2/bias','pred'\n",
    "    ]\n",
    "    \n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # The session is used to retrieve the weights\n",
    "        tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "        output_nodes_name # The output node names are used to select the usefull nodes\n",
    "    ) \n",
    "    \n",
    "    file = open('sign_lang_net.pb', \"wb\")\n",
    "    file.write(output_graph_def.SerializeToString())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " 'conv1_1/kernel/Initializer/Const',\n",
       " 'conv1_1/kernel',\n",
       " 'conv1_1/kernel/Assign',\n",
       " 'conv1_1/kernel/read',\n",
       " 'conv1_1/bias/Initializer/Const',\n",
       " 'conv1_1/bias',\n",
       " 'conv1_1/bias/Assign',\n",
       " 'conv1_1/bias/read',\n",
       " 'conv1_1/dilation_rate',\n",
       " 'conv1_1/Conv2D',\n",
       " 'conv1_1/BiasAdd',\n",
       " 'conv1_1/Relu',\n",
       " 'conv1_2/kernel/Initializer/Const',\n",
       " 'conv1_2/kernel',\n",
       " 'conv1_2/kernel/Assign',\n",
       " 'conv1_2/kernel/read',\n",
       " 'conv1_2/bias/Initializer/Const',\n",
       " 'conv1_2/bias',\n",
       " 'conv1_2/bias/Assign',\n",
       " 'conv1_2/bias/read',\n",
       " 'conv1_2/dilation_rate',\n",
       " 'conv1_2/Conv2D',\n",
       " 'conv1_2/BiasAdd',\n",
       " 'conv1_2/Relu',\n",
       " 'conv1_3/kernel/Initializer/Const',\n",
       " 'conv1_3/kernel',\n",
       " 'conv1_3/kernel/Assign',\n",
       " 'conv1_3/kernel/read',\n",
       " 'conv1_3/bias/Initializer/Const',\n",
       " 'conv1_3/bias',\n",
       " 'conv1_3/bias/Assign',\n",
       " 'conv1_3/bias/read',\n",
       " 'conv1_3/dilation_rate',\n",
       " 'conv1_3/Conv2D',\n",
       " 'conv1_3/BiasAdd',\n",
       " 'conv1_3/Relu',\n",
       " 'pool1/MaxPool',\n",
       " 'conv2_1/kernel/Initializer/Const',\n",
       " 'conv2_1/kernel',\n",
       " 'conv2_1/kernel/Assign',\n",
       " 'conv2_1/kernel/read',\n",
       " 'conv2_1/bias/Initializer/Const',\n",
       " 'conv2_1/bias',\n",
       " 'conv2_1/bias/Assign',\n",
       " 'conv2_1/bias/read',\n",
       " 'conv2_1/dilation_rate',\n",
       " 'conv2_1/Conv2D',\n",
       " 'conv2_1/BiasAdd',\n",
       " 'conv2_1/Relu',\n",
       " 'conv2_2/kernel/Initializer/Const',\n",
       " 'conv2_2/kernel',\n",
       " 'conv2_2/kernel/Assign',\n",
       " 'conv2_2/kernel/read',\n",
       " 'conv2_2/bias/Initializer/Const',\n",
       " 'conv2_2/bias',\n",
       " 'conv2_2/bias/Assign',\n",
       " 'conv2_2/bias/read',\n",
       " 'conv2_2/dilation_rate',\n",
       " 'conv2_2/Conv2D',\n",
       " 'conv2_2/BiasAdd',\n",
       " 'conv2_2/Relu',\n",
       " 'conv2_3/kernel/Initializer/Const',\n",
       " 'conv2_3/kernel',\n",
       " 'conv2_3/kernel/Assign',\n",
       " 'conv2_3/kernel/read',\n",
       " 'conv2_3/bias/Initializer/Const',\n",
       " 'conv2_3/bias',\n",
       " 'conv2_3/bias/Assign',\n",
       " 'conv2_3/bias/read',\n",
       " 'conv2_3/dilation_rate',\n",
       " 'conv2_3/Conv2D',\n",
       " 'conv2_3/BiasAdd',\n",
       " 'conv2_3/Relu',\n",
       " 'pool2/MaxPool',\n",
       " 'conv3_1/kernel/Initializer/Const',\n",
       " 'conv3_1/kernel',\n",
       " 'conv3_1/kernel/Assign',\n",
       " 'conv3_1/kernel/read',\n",
       " 'conv3_1/bias/Initializer/Const',\n",
       " 'conv3_1/bias',\n",
       " 'conv3_1/bias/Assign',\n",
       " 'conv3_1/bias/read',\n",
       " 'conv3_1/dilation_rate',\n",
       " 'conv3_1/Conv2D',\n",
       " 'conv3_1/BiasAdd',\n",
       " 'conv3_1/Relu',\n",
       " 'conv3_2/kernel/Initializer/Const',\n",
       " 'conv3_2/kernel',\n",
       " 'conv3_2/kernel/Assign',\n",
       " 'conv3_2/kernel/read',\n",
       " 'conv3_2/bias/Initializer/Const',\n",
       " 'conv3_2/bias',\n",
       " 'conv3_2/bias/Assign',\n",
       " 'conv3_2/bias/read',\n",
       " 'conv3_2/dilation_rate',\n",
       " 'conv3_2/Conv2D',\n",
       " 'conv3_2/BiasAdd',\n",
       " 'conv3_2/Relu',\n",
       " 'conv3_3/kernel/Initializer/Const',\n",
       " 'conv3_3/kernel',\n",
       " 'conv3_3/kernel/Assign',\n",
       " 'conv3_3/kernel/read',\n",
       " 'conv3_3/bias/Initializer/Const',\n",
       " 'conv3_3/bias',\n",
       " 'conv3_3/bias/Assign',\n",
       " 'conv3_3/bias/read',\n",
       " 'conv3_3/dilation_rate',\n",
       " 'conv3_3/Conv2D',\n",
       " 'conv3_3/BiasAdd',\n",
       " 'conv3_3/Relu',\n",
       " 'pool3/MaxPool',\n",
       " 'conv4_1/kernel/Initializer/Const',\n",
       " 'conv4_1/kernel',\n",
       " 'conv4_1/kernel/Assign',\n",
       " 'conv4_1/kernel/read',\n",
       " 'conv4_1/bias/Initializer/Const',\n",
       " 'conv4_1/bias',\n",
       " 'conv4_1/bias/Assign',\n",
       " 'conv4_1/bias/read',\n",
       " 'conv4_1/dilation_rate',\n",
       " 'conv4_1/Conv2D',\n",
       " 'conv4_1/BiasAdd',\n",
       " 'conv4_1/Relu',\n",
       " 'conv4_2/kernel/Initializer/Const',\n",
       " 'conv4_2/kernel',\n",
       " 'conv4_2/kernel/Assign',\n",
       " 'conv4_2/kernel/read',\n",
       " 'conv4_2/bias/Initializer/Const',\n",
       " 'conv4_2/bias',\n",
       " 'conv4_2/bias/Assign',\n",
       " 'conv4_2/bias/read',\n",
       " 'conv4_2/dilation_rate',\n",
       " 'conv4_2/Conv2D',\n",
       " 'conv4_2/BiasAdd',\n",
       " 'conv4_2/Relu',\n",
       " 'conv4_3/kernel/Initializer/Const',\n",
       " 'conv4_3/kernel',\n",
       " 'conv4_3/kernel/Assign',\n",
       " 'conv4_3/kernel/read',\n",
       " 'conv4_3/bias/Initializer/Const',\n",
       " 'conv4_3/bias',\n",
       " 'conv4_3/bias/Assign',\n",
       " 'conv4_3/bias/read',\n",
       " 'conv4_3/dilation_rate',\n",
       " 'conv4_3/Conv2D',\n",
       " 'conv4_3/BiasAdd',\n",
       " 'conv4_3/Relu',\n",
       " 'Mean/reduction_indices',\n",
       " 'Mean',\n",
       " 'Mean_1/reduction_indices',\n",
       " 'Mean_1',\n",
       " 'fc1/kernel/Initializer/Const',\n",
       " 'fc1/kernel',\n",
       " 'fc1/kernel/Assign',\n",
       " 'fc1/kernel/read',\n",
       " 'fc1/bias/Initializer/Const',\n",
       " 'fc1/bias',\n",
       " 'fc1/bias/Assign',\n",
       " 'fc1/bias/read',\n",
       " 'fc1/MatMul',\n",
       " 'fc1/BiasAdd',\n",
       " 'fc1/Relu',\n",
       " 'fc2/kernel/Initializer/Const',\n",
       " 'fc2/kernel',\n",
       " 'fc2/kernel/Assign',\n",
       " 'fc2/kernel/read',\n",
       " 'fc2/bias/Initializer/Const',\n",
       " 'fc2/bias',\n",
       " 'fc2/bias/Assign',\n",
       " 'fc2/bias/read',\n",
       " 'fc2/MatMul',\n",
       " 'fc2/BiasAdd',\n",
       " 'pred',\n",
       " 'init',\n",
       " 'init_1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = 'import/0:0'\n",
    "out_tensor = 'import/pred:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-e1184225ee4e>:2: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "[[ -3.2383754   32.943283    -6.156398   -18.473707    -2.4727986\n",
      "    0.27463734  -1.5059701  -14.443996   -26.715906    -7.621884  ]] ZERO\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with gfile.FastGFile(\"sign_lang_net.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        sess.graph.as_default()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = dict([(in_tensor, x.permute([0,2,3,1]))])\n",
    "        y = sess.run(out_tensor, feed_dict=feed_dict)\n",
    "        \n",
    "        print(y, label_dict[np.argmax(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to Tensoflow Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = '0'\n",
    "out_tensor = 'pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5098924"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_def_file = \"sign_lang_net.pb\"\n",
    "input_arrays = [in_tensor]\n",
    "output_arrays = [out_tensor]\n",
    "\n",
    "converter = TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"sign_lang_net.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
